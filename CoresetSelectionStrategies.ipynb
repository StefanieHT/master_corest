{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb05786-23b6-4a6c-b5c3-de096ac52eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "!pip install -U git+https://github.com/albu/albumentations --no-cache-di\n",
    "!pip install segmentation-models-pytorch\n",
    "!pip install psutil\n",
    "!pip install --extra-index-url=https://pypi.nvidia.com cudf-cu12==24.8.* dask-cudf-cu12==24.8.* cuml-cu12==24.8.* cugraph-cu12==24.8.* cuspatial-cu12==24.8.* cuproj-cu12==24.8.* cuxfilter-cu12==24.8.* cucim-cu12==24.8.* pylibraft-cu12==24.8.* raft-dask-cu12==24.8.* cuvs-cu12==24.8.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f99e8b-0a34-41f4-9398-a2ecaa23e992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1f759-1829-46b8-8042-0265c442996c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a new dataset to only return images (without masks)\n",
    "class ImageDataset:\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataset = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        df_row = self.dataset.iloc[index]\n",
    "        vv_image = cv2.imread(df_row['vv_image_path'], 0) / 255.0\n",
    "        vh_image = cv2.imread(df_row['vh_image_path'], 0) / 255.0\n",
    "        rgb_image = s1_to_rgb(vv_image, vh_image)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=rgb_image)\n",
    "            rgb_image = augmented['image']\n",
    "\n",
    "        return {'image': rgb_image.transpose((2,0,1)).astype('float32')}\n",
    "\n",
    "\n",
    "class ETCIDataset(Dataset):\n",
    "    def __init__(self, dataframe, split, transform=None):\n",
    "        self.split = split\n",
    "        self.dataset = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "\n",
    "        df_row = self.dataset.iloc[index]\n",
    "\n",
    "        # load vv and vh images\n",
    "        vv_image = cv2.imread(df_row['vv_image_path'], 0) / 255.0\n",
    "        vh_image = cv2.imread(df_row['vh_image_path'], 0) / 255.0\n",
    "\n",
    "        # convert vv and ch images to rgb\n",
    "        rgb_image = s1_to_rgb(vv_image, vh_image)\n",
    "\n",
    "        if self.split == 'test':\n",
    "            # no flood mask should be available\n",
    "            example['image'] = rgb_image.transpose((2,0,1)).astype('float32')\n",
    "        else:\n",
    "            # load ground truth flood mask\n",
    "            flood_mask = cv2.imread(df_row['flood_label_path'], 0) / 255.0\n",
    "\n",
    "            # compute transformations\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=rgb_image, mask=flood_mask)\n",
    "                rgb_image = augmented['image']\n",
    "                flood_mask = augmented['mask']\n",
    "\n",
    "            example['image'] = rgb_image.transpose((2,0,1)).astype('float32')\n",
    "            example['mask'] = flood_mask.astype('int64')\n",
    "\n",
    "        return example\n",
    "\n",
    "def prepare_dataset_paths(dset_root):\n",
    "    train_dir = os.path.join(dset_root, 'train')\n",
    "    n_train_regions = len(glob(train_dir + '/*/'))\n",
    "\n",
    "    vv_image_paths = sorted(glob(train_dir + '/**/vv/*.png', recursive=True))\n",
    "    vv_image_names = [get_filename(pth) for pth in vv_image_paths]\n",
    "    region_name_dates = ['_'.join(n.split('_')[:2]) for n in vv_image_names]\n",
    "\n",
    "    vh_image_paths, flood_label_paths, region_names = [], [], []\n",
    "    for i in range(len(vv_image_paths)):\n",
    "        vh_image_name = vv_image_names[i].replace('vv', 'vh')\n",
    "        vh_image_path = os.path.join(train_dir, region_name_dates[i], 'tiles', 'vh', vh_image_name)\n",
    "        vh_image_paths.append(vh_image_path)\n",
    "\n",
    "        flood_image_name = vv_image_names[i].replace('_vv', '')\n",
    "        flood_label_path = os.path.join(train_dir, region_name_dates[i], 'tiles', 'flood_label', flood_image_name)\n",
    "        flood_label_paths.append(flood_label_path)\n",
    "\n",
    "        region_name = region_name_dates[i].split('_')[0]\n",
    "        region_names.append(region_name)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'vv_image_path': vv_image_paths,\n",
    "        'vh_image_path': vh_image_paths,\n",
    "        'flood_label_path': flood_label_paths,\n",
    "        'region': region_names\n",
    "    })\n",
    "\n",
    "\n",
    "def get_data_subset(train_df, coreset_path=None):\n",
    "    sub_train_df, _ = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    sub_train_df = sub_train_df.reset_index(drop=False)\n",
    "    \n",
    "    if coreset_path is not None:\n",
    "        with open(coreset_path, 'r') as json_file:\n",
    "            coreset_indices = json.load(json_file)\n",
    "        print(f\"coreset size: {len(coreset_indices)}\")\n",
    "        sub_train_df = sub_train_df[sub_train_df['index'].isin(coreset_indices)]\n",
    "   \n",
    "    return sub_train_df\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader(sub_train_df, batch_size, transform=None, coreset_indices=None, strategy=\"kmeans\"):\n",
    "    \n",
    "    train_df = sub_train_df\n",
    "    \n",
    "    if strategy == \"kmeans\":\n",
    "        train_dataset = ImageDataset(train_df)\n",
    "    else:\n",
    "        train_dataset = ETCIDataset(train_df, split='train', transform=None)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return generator\n",
    "\n",
    "\n",
    "def get_filename(filepath):\n",
    "    return os.path.split(filepath)[1]\n",
    "\n",
    "def s1_to_rgb(vv_image, vh_image):\n",
    "    ratio_image = np.clip(np.nan_to_num(vh_image/vv_image, 0), 0, 1)\n",
    "    rgb_image = np.stack((vv_image, vh_image, 1-ratio_image), axis=2)\n",
    "    return rgb_image\n",
    "\n",
    "\n",
    "def create_model(device='cuda'):\n",
    "    # load model from package\n",
    "    model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        \n",
    "    encoder_weights=None,           \n",
    "    in_channels=3,                  \n",
    "    classes=2,                      \n",
    "    )\n",
    "    model.load_state_dict(torch.load('model_test_lr_5epochs1.pt')) # load pretrained model\n",
    "\n",
    "    model.to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81d50f-bc33-4731-8a36-4efda49b3d71",
   "metadata": {},
   "source": [
    "# KMeans Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dd0e4-920c-4eb3-8b18-bb37f8fab95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from cuml.cluster import KMeans\n",
    "from cuml.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from cuml.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Extracting Features\")):\n",
    "            images = batch['image'].to(device)\n",
    "\n",
    "            # Extract encoder features (last encoder block)\n",
    "            encoder_features = model.encoder(images)[-1]  \n",
    "\n",
    "            features.append(encoder_features.cpu().numpy())\n",
    "\n",
    "    concatenated_features = np.concatenate(features, axis=0)  # Shape (total_samples, channels, H, W)\n",
    "    \n",
    "    return concatenated_features\n",
    "\n",
    "def apply_pca(features, n_components=256):\n",
    "    num_samples, channels, height, width = features.shape  \n",
    "    \n",
    "    reshaped_features = features.transpose(0, 2, 3, 1).reshape(-1, channels) \n",
    "        \n",
    "    pca = PCA(n_components=n_components)\n",
    "    features_pca = pca.fit_transform(reshaped_features) )\n",
    "    \n",
    "    features_pca = features_pca.reshape(num_samples, height, width, n_components)\n",
    "    \n",
    "    features_pca = features_pca.reshape(num_samples, -1)\n",
    "    \n",
    "    return features_pca\n",
    "\n",
    "\n",
    "\n",
    "def create_clusters(n_clusters, features):\n",
    "    print(\"\\nStarting clustering process... \")\n",
    "    print(f\"Feature shape: {features.shape}\")\n",
    "    print(f\"Expected number of clusters: {n_clusters}\")\n",
    "\n",
    "    features = np.nan_to_num(features).astype(np.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    kmeans_cuml = KMeans(n_clusters=n_clusters, random_state=3)\n",
    "    kmeans_cuml.fit(features)\n",
    "\n",
    "    cluster_centers = kmeans_cuml.cluster_centers_\n",
    "    cluster_labels = kmeans_cuml.labels_\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"Time taken for clustering: {end_time:.2f} seconds\")\n",
    "\n",
    "    return cluster_centers, cluster_labels\n",
    "\n",
    "\n",
    "def find_cluster_centers_balanced(features, cluster_centers, cluster_labels, n_clusters):\n",
    "    coreset_indices = []\n",
    "    cluster_samples = {i: [] for i in range(n_clusters)}\n",
    "    \n",
    "    # Group samples by cluster\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        cluster_samples[label].append(idx)\n",
    "\n",
    "    # Identify non-empty clusters\n",
    "    non_empty_clusters = {k: v for k, v in cluster_samples.items() if len(v) > 0}\n",
    "    \n",
    "    # Select one point per non-empty cluster (closest to center)\n",
    "    for cluster_label, cluster_indices in non_empty_clusters.items():\n",
    "        center = cluster_centers[cluster_label]\n",
    "        distances = np.linalg.norm(features[cluster_indices] - center, axis=1)\n",
    "        closest_index = cluster_indices[np.argmin(distances)]\n",
    "        coreset_indices.append(closest_index)\n",
    "    \n",
    "    # Fill in extra selections using size-weighted sampling from non-empty clusters\n",
    "    remaining = n_clusters - len(coreset_indices)\n",
    "    if remaining > 0:\n",
    "        # Compute selection weights based on cluster sizes\n",
    "        cluster_ids = list(non_empty_clusters.keys())\n",
    "        sizes = np.array([len(non_empty_clusters[k]) for k in cluster_ids])\n",
    "        probabilities = sizes / sizes.sum()\n",
    "        \n",
    "        attempts = 0\n",
    "        while len(coreset_indices) < n_clusters and attempts < 1000:\n",
    "            selected_cluster = np.random.choice(cluster_ids, p=probabilities)\n",
    "            candidates = list(set(non_empty_clusters[selected_cluster]) - set(coreset_indices))\n",
    "            if candidates:\n",
    "                center = cluster_centers[selected_cluster]\n",
    "                distances = np.linalg.norm(features[candidates] - center, axis=1)\n",
    "                closest_index = candidates[np.argmin(distances)]\n",
    "                coreset_indices.append(closest_index)\n",
    "            attempts += 1\n",
    "   \n",
    "    return coreset_indices\n",
    "\n",
    "\n",
    "\n",
    "def save_kmeans_coreset(cluster_indices_lists, sub_train_df, filename='samples_kmeans'):\n",
    "    cluster_indices_lists = sub_train_df[\"index\"].iloc[cluster_indices_lists].tolist()\n",
    "\n",
    "    # Save the final refined core set indices\n",
    "    with open(filename+'.json', 'w') as json_file:\n",
    "        json.dump(cluster_indices_lists, json_file)\n",
    "\n",
    "\n",
    "def run_kmeans_sampling(subset_input_path=None, coreset_size=1, coreset_output_path=\"samples_kmeans1\"):\n",
    "    dset_root = 'dataset/ETCI_2021_Competition_Dataset/'\n",
    "    train_df = prepare_dataset_paths(dset_root)\n",
    "    full_train_data_len = round(len(train_df)*0.8)\n",
    "\n",
    "    sub_train_df = get_data_subset(train_df, coreset_path=subset_input_path)\n",
    "\n",
    "\n",
    "    image_loader = get_dataloader(\n",
    "        sub_train_df,\n",
    "        batch_size=64,\n",
    "        transform=None,\n",
    "    )\n",
    "\n",
    "    device = 'cuda'\n",
    "    model = create_model(device)\n",
    "    features = extract_features(model, image_loader, device)\n",
    "\n",
    "    cluster_num = round(full_train_data_len*coreset_size)\n",
    "    print(f\" Number of clusters: {cluster_num}\")\n",
    "\n",
    "    features = np.nan_to_num(features).astype(np.float32)\n",
    "\n",
    "    features_pca = apply_pca(features, n_components=128)\n",
    "\n",
    "    # Run clustering\n",
    "    cluster_centers, cluster_labels = create_clusters(cluster_num, features_pca)\n",
    "\n",
    "    cluster_indices_lists = find_cluster_centers_balanced(features_pca, cluster_centers, cluster_labels, cluster_num)\n",
    "\n",
    "    save_kmeans_coreset(cluster_indices_lists, sub_train_df, coreset_output_path)\n",
    "\n",
    "    print(f\"Core set indices saved to {coreset_output_path}.\")\n",
    "    print(\"Processing completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4989350-d34c-4a2c-9d4c-fc1fac4a71f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_kmeans_sampling(subset_input_path=\"samples/samples_encoder_full_weights_2.json\", coreset_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8ea3d-e71d-4cf0-96e8-24ccadcc4bb3",
   "metadata": {},
   "source": [
    "# Gradient Embedding Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590458e-4351-4df9-a76f-4ced9d8fe907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda'\n",
    "model = create_model(device)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92d99a-164d-4e1e-b478-088073d2f058",
   "metadata": {},
   "source": [
    "### Gradient Embeddings from last Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d880571-5ca8-428d-a8e1-9728a39b0b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_grad_embedding_last_encoder_layer(train_dataset, model, criterion, device='cuda'):\n",
    "    torch.cuda.empty_cache()\n",
    "    logger = logging.getLogger('GradientEmbeddingLogger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    grad_norms = []  \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)  # Batch size 1\n",
    "\n",
    "\n",
    "    # Manually specify the last encoder layer's parameters\n",
    "    last_encoder_layer = model.encoder.layer4[2].conv2  \n",
    "    last_encoder_params = list(last_encoder_layer.parameters())[0]  \n",
    "\n",
    "\n",
    "    for sample_idx, batch in enumerate(tqdm(loader, desc=\"Computing Gradient Embeddings\", leave=True)):\n",
    "        image = batch['image'].to(device)  # Single sample\n",
    "        mask = batch['mask'].to(device)  # Ground truth segmentation mask\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(image)\n",
    "\n",
    "        loss = criterion(logits, mask)  \n",
    "\n",
    "        loss_scalar = loss.mean()\n",
    "\n",
    "        last_encoder_params.requires_grad_(True)\n",
    "\n",
    "        # Compute gradient w.r.t. the last encoder layer\n",
    "        grad_embedding = torch.autograd.grad(loss_scalar, last_encoder_params, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "        # Compute L2 norm of the gradient embedding\n",
    "        grad_norm = torch.norm(grad_embedding, p=2).item()\n",
    "        grad_norms.append(grad_norm)\n",
    "\n",
    "\n",
    "    return grad_norms \n",
    "\n",
    "\n",
    "def select_most_influential_samples_encoder(grad_norms, sub_train_df, coreset_size=1, sample_path=\"coreset_encoder\"):\n",
    "    sub_train_df = sub_train_df.copy() \n",
    "    sub_train_df[\"gradient_norm\"] = grad_norms\n",
    "\n",
    "    # Sort the dataframe in descending order based on gradient_norm\n",
    "    sorted_df = sub_train_df.sort_values(by=\"gradient_norm\", ascending=False)\n",
    "    sorted_indices = sorted_df[\"index\"].tolist()\n",
    "    sorted_indices = sorted_indices[:coreset_size]\n",
    "    with open(sample_path+'.json', 'w') as json_file:\n",
    "       json.dump(sorted_indices, json_file)\n",
    "    \n",
    "def run_encoder_ge_sampling(subset_input_path=None, coreset_size=1, coreset_output_path=\"coreset_encoder\"):\n",
    "    dset_root = 'dataset/ETCI_2021_Competition_Dataset/'\n",
    "    train_df = prepare_dataset_paths(dset_root)\n",
    "    full_train_data_len = round(len(train_df)*0.8)\n",
    "\n",
    "    sub_df = get_data_subset(train_df, coreset_path=subset_input_path)\n",
    "    print(len(sub_df))\n",
    "    sub_dataset = ETCIDataset(sub_df, split='train', transform=None)\n",
    "    coreset_size = round(full_train_data_len*coreset_size)\n",
    "    grad_norms = get_grad_embedding_last_encoder_layer(sub_dataset, model, criterion, 'cuda')\n",
    "    select_most_influential_samples_encoder(grad_norms, sub_df, coreset_size=coreset_size, sample_path=coreset_output_path)\n",
    "    print(\"coreset selection finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a99ee0-b432-4337-bf63-9862098ccdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_encoder_ge_sampling(subset_input_path=\"samples/samples_kmeans_03.json\", coreset_size=0.1, coreset_output_path=\"coreset_encoder1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c5ff6-21ae-437e-b1c5-8e9ccee94bc1",
   "metadata": {},
   "source": [
    "### Gradient Embeddings from last Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c8f18-6ade-4ce5-bb73-2d51d6960f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "def get_grad_embedding_last_decoder_layer(train_dataset, model, criterion, device='cuda'):\n",
    "    torch.cuda.empty_cache()\n",
    "    logger = logging.getLogger('GradientEmbeddingLogger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    grad_norms = []  \n",
    "    model.train()\n",
    "\n",
    "    loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)  # Batch size 1\n",
    "\n",
    "    logger.info(f'Starting gradient embedding computation for {len(train_dataset)} samples.')\n",
    "\n",
    "    # Manually specify the last decoder layer's parameters\n",
    "    last_decoder_layer = model.decoder.blocks[4].conv2[0]  \n",
    "    last_decoder_params = list(last_decoder_layer.parameters())[0]  \n",
    "\n",
    "    logger.info(f\"Using last decoder layer: decoder.blocks[4].conv2[0]\")\n",
    "\n",
    "    for sample_idx, batch in enumerate(tqdm(loader, desc=\"Computing Gradient Embeddings\", leave=True)):\n",
    "        image = batch['image'].to(device)  # Single sample\n",
    "        mask = batch['mask'].to(device)  # Ground truth segmentation mask\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(image)\n",
    "\n",
    "        loss = criterion(logits, mask)  \n",
    "\n",
    "        loss_scalar = loss.mean()\n",
    "\n",
    "        last_decoder_params.requires_grad_(True)\n",
    "\n",
    "        # Compute gradient w.r.t. the last decoder layer\n",
    "        grad_embedding = torch.autograd.grad(loss_scalar, last_decoder_params, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "        if grad_embedding is None:\n",
    "            logger.error(f\"No gradients computed for sample {sample_idx}. Check requires_grad settings.\")\n",
    "            continue\n",
    "\n",
    "        # Compute L2 norm of the gradient embedding\n",
    "        grad_norm = torch.norm(grad_embedding, p=2).item()\n",
    "        grad_norms.append(grad_norm)\n",
    "\n",
    "    logger.info('Completed gradient embedding computation.')\n",
    "\n",
    "    return grad_norms  # List of L2 norms, one per sample\n",
    "\n",
    "def select_most_influential_samples_decoder(grad_norms, sub_train_df, coreset_size=1, sample_path=\"coreset_encoder\"):\n",
    "    sub_train_df = sub_train_df.copy() \n",
    "    sub_train_df[\"gradient_norm\"] = grad_norms\n",
    "\n",
    "    # Sort the dataframe in descending order based on gradient_norm\n",
    "    sorted_df = sub_train_df.sort_values(by=\"gradient_norm\", ascending=False)\n",
    "    sorted_indices = sorted_df[\"index\"].tolist()\n",
    "    sorted_indices = sorted_indices[:coreset_size]\n",
    "    with open(sample_path+'.json', 'w') as json_file:\n",
    "       json.dump(sorted_indices, json_file)\n",
    "    \n",
    "def run_decoder_ge_sampling(subset_input_path=None, coreset_size=1, coreset_output_path=\"coreset_encoder\"):\n",
    "    dset_root = 'dataset/ETCI_2021_Competition_Dataset/'\n",
    "    train_df = prepare_dataset_paths(dset_root)\n",
    "    full_train_data_len = round(len(train_df)*0.8)\n",
    "\n",
    "    sub_df = get_data_subset(train_df, coreset_path=subset_input_path)\n",
    "    print(len(sub_df))\n",
    "    sub_dataset = ETCIDataset(sub_df, split='train', transform=None)\n",
    "    coreset_size = round(full_train_data_len*coreset_size)\n",
    "    grad_norms = get_grad_embedding_last_decoder_layer(sub_dataset, model, criterion, 'cuda')\n",
    "    select_most_influential_samples_decoder(grad_norms, sub_df, coreset_size=coreset_size, sample_path=coreset_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99912e3a-33d3-473a-a9d0-75906954eeeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_decoder_ge_sampling(coreset_size=1, coreset_output_path=\"coreset_decoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62625ce1-4d9e-4508-ad6f-3306da7a3a7b",
   "metadata": {},
   "source": [
    "Note: For the Hybrid Sampling methods, simply run either KMeans or Gradient Samppling first to save the subset for the first stage. Then load the indices of the subset for the other Sampling method in the second stage to use as the starting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa6918-f824-4f60-acee-ead62e9476f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
